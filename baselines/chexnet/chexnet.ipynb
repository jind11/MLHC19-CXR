{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "from skimage.external import tifffile\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd\n",
    "from ChexnetTrainer import ChexnetTrainer\n",
    "import torch\n",
    "\n",
    "working_dir = '/mnt/storage/mlhc_project'\n",
    "def bdir(fname):\n",
    "    return os.path.join(working_dir, fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_4/data/tiffs_20k_4\n",
      "data_5/data/tiffs_5\n",
      "data_6/data/tiffs_6\n",
      "data_2/data/tiffs_2\n",
      "data_7/data/tiffs_7\n",
      "data_8/data/tiffs_8\n"
     ]
    }
   ],
   "source": [
    "# Need to generate the txt file for train and validation\n",
    "txt_file_path = './dataset/train.txt'\n",
    "labels_paths = ['../data_4/labels_20k_4.npy', \n",
    "           '../data_5/labels_5.npy', \n",
    "           '../data_6/labels_6.npy',\n",
    "               '../data_2/labels_2.npy',\n",
    "               '../data_7/labels_7.npy',\n",
    "               '../data_8/labels_8.npy']\n",
    "img_dirs = ['../data_4/data/tiffs_20k_4', \n",
    "           '../data_5/data/tiffs_5', \n",
    "           '../data_6/data/tiffs_6',\n",
    "           '../data_2/data/tiffs_2',\n",
    "           '../data_7/data/tiffs_7',\n",
    "           '../data_8/data/tiffs_8']\n",
    "\n",
    "########################\n",
    "\n",
    "labels = []\n",
    "for labels_path in labels_paths:\n",
    "    labels.append(np.load(labels_path, encoding='bytes'))\n",
    "total_labels = np.concatenate(tuple(labels), axis=0)\n",
    "\n",
    "\n",
    "names_list = []\n",
    "for img_dir in img_dirs:\n",
    "    temp_names_list = [' ']*len(os.listdir(img_dir))\n",
    "    temp_dir_header = img_dir[3:]\n",
    "    print(temp_dir_header)\n",
    "    for fname in os.listdir(img_dir):\n",
    "        # Get actual index \n",
    "        index = int(fname.split('_')[0])\n",
    "        temp_names_list[index] = os.path.join(temp_dir_header, fname)\n",
    "    names_list += temp_names_list\n",
    "    \n",
    "d = {'filename':names_list}\n",
    "df1 = pd.DataFrame(data=d)\n",
    "df2 = pd.DataFrame(data=total_labels)\n",
    "df2 = df2.astype('int')\n",
    "\n",
    "df = pd.concat((df1,df2),axis=1)\n",
    "\n",
    "df.to_csv('./dataset/train.txt',sep=' ', index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Need to generate the txt file for train and validation\n",
    "txt_file_path = './dataset/valtest.txt'\n",
    "labels_path = '../data_3/labels_20k_3.npy'\n",
    "img_dir = '../data_3/data/tiffs_3'\n",
    "\n",
    "########################\n",
    "labels = np.load(labels_path, encoding='bytes')\n",
    "#np.save(labels_path, labels.astype('int'))\n",
    "names_list = [' ']*len(labels)\n",
    "for fname in os.listdir(img_dir):\n",
    "    # Get actual index \n",
    "    index = int(fname.split('_')[0])\n",
    "    names_list[index] = fname\n",
    "    \n",
    "d = {'filename':names_list}\n",
    "df1 = pd.DataFrame(data=d)\n",
    "df2 = pd.DataFrame(data=labels)\n",
    "df2 = df2.astype('int')\n",
    "\n",
    "df = pd.concat((df1,df2),axis=1)\n",
    "\n",
    "df.to_csv(txt_file_path,sep=' ', index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert valtest into val and test\n",
    "val_txt_file_path = './dataset/val.txt'\n",
    "test_txt_file_path = './dataset/test.txt'\n",
    "index_to_split = 8712\n",
    "\n",
    "df = pd.read_csv(txt_file_path, sep=' ', header=None)\n",
    "df_val = df[:index_to_split]\n",
    "df_test = df[index_to_split:]\n",
    "df_val.to_csv(val_txt_file_path, sep=' ', index=False, header=False)\n",
    "df_test.to_csv(test_txt_file_path, sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training code \n",
    "def runTrain():\n",
    "    checkpoint = 'models/m-09052019-153716.pth.tar' # if None, then we start training over. otherwise start training an old model \n",
    "    DENSENET121 = 'DENSE-NET-121'\n",
    "    DENSENET169 = 'DENSE-NET-169'\n",
    "    DENSENET201 = 'DENSE-NET-201'\n",
    "    \n",
    "    timestampTime = time.strftime(\"%H%M%S\")\n",
    "    timestampDate = time.strftime(\"%d%m%Y\")\n",
    "    timestampLaunch = timestampDate + '-' + timestampTime\n",
    "    \n",
    "    #---- Path to the directory with images\n",
    "#     pathDirDataTrain = [bdir('data/dataset_20k.npy'), bdir('data/labels_20k.npy')]\n",
    "#     pathDirDataVal = [bdir('data_2/dataset_20k_2.npy'), bdir('data_2/labels_20k_2.npy')]\n",
    "#     pathDirDataTrain = bdir('data_4/data/tiffs_20k_4')\n",
    "    pathDirDataTrain = bdir('')\n",
    "    pathDirDataVal = bdir('data_3/data/tiffs_3')\n",
    "#     pathDirDataVal = None\n",
    "    \n",
    "    #---- Paths to the files with training, validation and testing sets.\n",
    "    #---- Each file should contains pairs [path to image, output vector]\n",
    "    #---- Example: images_011/00027736_001.png 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
    "    pathFileTrain = 'dataset/train.txt'\n",
    "    pathFileVal = 'dataset/val.txt'\n",
    "    #pathFileTest = './dataset/test_1.txt'\n",
    "    \n",
    "    #---- Neural network parameters: type of the network, is it pre-trained \n",
    "    #---- on imagenet, number of classes\n",
    "    nnArchitecture = DENSENET121\n",
    "    nnIsTrained = True\n",
    "    nnClassCount = 14\n",
    "    \n",
    "    #---- Training settings: batch size, maximum number of epochs\n",
    "    trBatchSize = 16\n",
    "    trMaxEpoch = 50\n",
    "    \n",
    "    #---- Parameters related to image transforms: size of the down-scaled image, cropped image\n",
    "    imgtransResize = 224\n",
    "    imgtransCrop = 224\n",
    "        \n",
    "    pathModel = 'm-' + timestampLaunch + '.pth.tar'\n",
    "    \n",
    "    print ('Training NN architecture = ', nnArchitecture)\n",
    "    ChexnetTrainer.train(pathDirDataTrain, pathDirDataVal, pathFileTrain, pathFileVal, nnArchitecture, nnIsTrained, nnClassCount,\n",
    "                         trBatchSize, trMaxEpoch, imgtransResize, imgtransCrop, timestampLaunch, checkpoint)\n",
    "    \n",
    "#     print ('Testing the trained model')\n",
    "#     ChexnetTrainer.test(pathDirData, pathFileTest, pathModel, nnArchitecture, nnClassCount, nnIsTrained, trBatchSize, imgtransResize, imgtransCrop, timestampLaunch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training NN architecture =  DENSE-NET-121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:05,  4.35it/s]\n",
      "545it [00:13, 40.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1] [save] [10052019-030645] val_loss= 0.26089737025012666, train_loss= 0.2427527137126423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:17, 11.64it/s]\n",
      "545it [00:14, 38.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2] [save] [10052019-031823] val_loss= 0.2602634498832423, train_loss= 0.24285946641776712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:25,  9.72it/s]\n",
      "545it [00:14, 38.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3] [----] [10052019-033008] val_loss= 0.2613232410555586, train_loss= 0.24267060871490073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:17, 10.76it/s]\n",
      "545it [00:14, 38.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4] [----] [10052019-034147] val_loss= 0.26163316952799437, train_loss= 0.24250983269324516\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:25, 10.37it/s]\n",
      "545it [00:14, 38.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5] [----] [10052019-035334] val_loss= 0.2615101917006007, train_loss= 0.24249618018940572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:24,  9.73it/s]\n",
      "545it [00:14, 38.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6] [----] [10052019-040520] val_loss= 0.2604652749835898, train_loss= 0.2422971085805629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:28, 10.31it/s]\n",
      "545it [00:13, 39.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7] [----] [10052019-041711] val_loss= 0.26069610081271294, train_loss= 0.24242554197859395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:26,  9.70it/s]\n",
      "545it [00:13, 47.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8] [save] [10052019-042857] val_loss= 0.25985545078548816, train_loss= 0.24216216120400813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:21, 10.78it/s]\n",
      "545it [00:14, 38.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9] [----] [10052019-044040] val_loss= 0.26180722755849906, train_loss= 0.24206488259162404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:28, 11.16it/s]\n",
      "545it [00:13, 46.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10] [----] [10052019-045231] val_loss= 0.2602139878642122, train_loss= 0.24201807437551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:20,  9.79it/s]\n",
      "545it [00:13, 40.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11] [----] [10052019-050411] val_loss= 0.2609817937494965, train_loss= 0.24195267961957115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:14,  9.86it/s]\n",
      "545it [00:13, 39.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12] [----] [10052019-051546] val_loss= 0.2614573829320319, train_loss= 0.2418460471382497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:27, 10.59it/s]\n",
      "545it [00:13, 40.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13] [----] [10052019-052734] val_loss= 0.2610731567469759, train_loss= 0.2419770126990362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:22,  9.76it/s]\n",
      "545it [00:14, 37.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14] [----] [10052019-053918] val_loss= 0.2616229837435648, train_loss= 0.24154618763875324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:23, 10.79it/s]\n",
      "545it [00:13, 47.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15] [----] [10052019-055103] val_loss= 0.26106386784156527, train_loss= 0.24171015950424002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:22,  9.76it/s]\n",
      "545it [00:13, 40.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16] [----] [10052019-060246] val_loss= 0.26027381452261855, train_loss= 0.24189492972866047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:16, 10.50it/s]\n",
      "545it [00:14, 38.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17] [----] [10052019-061425] val_loss= 0.26077836715710273, train_loss= 0.2416027405811956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:23, 10.86it/s]\n",
      "545it [00:14, 38.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18] [----] [10052019-062608] val_loss= 0.2601851441786377, train_loss= 0.24139632563269042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:25,  9.93it/s]\n",
      "545it [00:14, 43.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19] [----] [10052019-063755] val_loss= 0.26024721759038233, train_loss= 0.2417298925498319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:24,  9.73it/s]\n",
      "545it [00:14, 38.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20] [----] [10052019-064940] val_loss= 0.2610368210104627, train_loss= 0.2414881917612364\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6661it [11:27,  9.68it/s]\n",
      "545it [00:12, 42.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21] [----] [10052019-070129] val_loss= 0.26003154153260616, train_loss= 0.2414450574363697\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2617it [04:26,  8.93it/s]"
     ]
    }
   ],
   "source": [
    "runTrain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def runTest():\n",
    "    \n",
    "    # Test set \n",
    "    pathDirData = bdir('data_3/data/tiffs_3')\n",
    "    pathFileTest = 'dataset/test.txt'\n",
    "    nnArchitecture = 'DENSE-NET-121'\n",
    "    nnIsTrained = True\n",
    "    nnClassCount = 14\n",
    "    trBatchSize = 16\n",
    "    imgtransResize = 224\n",
    "    imgtransCrop = 224\n",
    "    \n",
    "    # 'models/m-08052019-184741.pth.tar' is the 20k model \n",
    "    pathModel = 'models/m-08052019-184741.pth.tar'\n",
    "\n",
    "    \n",
    "    timestampLaunch = ''\n",
    "    print(\"Test set:\")\n",
    "    ChexnetTrainer.test(pathDirData, pathFileTest, pathModel, nnArchitecture, nnClassCount, nnIsTrained, trBatchSize, imgtransResize, imgtransCrop, timestampLaunch)\n",
    "    \n",
    "    \n",
    "    # Val set \n",
    "    pathDirData = bdir('data_3/data/tiffs_3')\n",
    "    pathFileTest = 'dataset/val.txt'\n",
    "    print(\"Val set:\")\n",
    "    ChexnetTrainer.test(pathDirData, pathFileTest, pathModel, nnArchitecture, nnClassCount, nnIsTrained, trBatchSize, imgtransResize, imgtransCrop, timestampLaunch)\n",
    "    \n",
    "    \n",
    "    # Train set \n",
    "    pathDirData = bdir('')\n",
    "    pathFileTest = 'dataset/train.txt'\n",
    "    print(\"Train set:\")\n",
    "    ChexnetTrainer.test(pathDirData, pathFileTest, pathModel, nnArchitecture, nnClassCount, nnIsTrained, trBatchSize, imgtransResize, imgtransCrop, timestampLaunch)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/webster/anaconda3/envs/hubris/lib/python3.6/site-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Finding   0.8471829900693314\n",
      "Enlarged Cardiomediastinum   0.6878402575717024\n",
      "Cardiomegaly   0.7920743586275965\n",
      "Airspace Opacity   0.7293064029579733\n",
      "Lung Lesion   0.6971406003159558\n",
      "Edema   0.8759862186593894\n",
      "Consolidation   0.779920146475776\n",
      "Pneumonia   0.73054510033367\n",
      "Atelectasis   0.7977155500733691\n",
      "Pneumothorax   0.7940490516848622\n",
      "Pleural Effusion   0.901310436177698\n",
      "Pleural Other   0.787115292286807\n",
      "Fracture   0.6463964316967726\n",
      "Support Devices   0.8714726586320809\n",
      "AUROC mean  0.7812810643684784\n",
      "Val set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/webster/anaconda3/envs/hubris/lib/python3.6/site-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Finding   0.8515557042175443\n",
      "Enlarged Cardiomediastinum   0.7136849900647442\n",
      "Cardiomegaly   0.7954346129374082\n",
      "Airspace Opacity   0.7367577400150691\n",
      "Lung Lesion   0.69655310560055\n",
      "Edema   0.8762558122494\n",
      "Consolidation   0.78930960335876\n",
      "Pneumonia   0.710487906491001\n",
      "Atelectasis   0.7959914912016632\n",
      "Pneumothorax   0.8229949115580325\n",
      "Pleural Effusion   0.9029066522564707\n",
      "Pleural Other   0.7837479655893977\n",
      "Fracture   0.6047700203729822\n",
      "Support Devices   0.8765291033236826\n",
      "AUROC mean  0.7826776915364944\n",
      "Train set:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/webster/anaconda3/envs/hubris/lib/python3.6/site-packages/torchvision/models/densenet.py:212: UserWarning: nn.init.kaiming_normal is now deprecated in favor of nn.init.kaiming_normal_.\n",
      "  nn.init.kaiming_normal(m.weight.data)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1014e5ac8211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrunTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-2415e7251645>\u001b[0m in \u001b[0;36mrunTest\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mpathFileTest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dataset/train.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train set:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mChexnetTrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathDirData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathFileTest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnArchitecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnClassCount\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnnIsTrained\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrBatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgtransResize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgtransCrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimestampLaunch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/storage/mlhc_project/chexnet/ChexnetTrainer.py\u001b[0m in \u001b[0;36mtest\u001b[0;34m(pathDirData, pathFileTest, pathModel, nnArchitecture, nnClassCount, nnIsTrained, trBatchSize, transResize, transCrop, launchTimeStamp)\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataLoaderTest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m                 \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m                 \u001b[0moutGT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutGT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "runTest()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
