{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='start'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Attempt to implement Tienet in a day\n",
    "import torchvision\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import transforms\n",
    "import os, time\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import numpy as np \n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline \n",
    "from sklearn.metrics.ranking import roc_auc_score\n",
    "\n",
    "from DatasetGenerator import *\n",
    "from ChexnetTrainer import masked_BCE, DataLoader, TieNetLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump to training if functions have already been loaded](#train)<br><br>\n",
    "\n",
    "[Jump to training if functions haven't been loaded](#train2)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We first make the train and valtest .txt files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_4/data/tiffs_20k_4\n",
      "data_5/data/tiffs_5\n",
      "data_6/data/tiffs_6\n",
      "data_2/data/tiffs_2\n",
      "data_7/data/tiffs_7\n",
      "data_8/data/tiffs_8\n",
      "data_4/reports_20k_4\n",
      "data_5/reports_5\n",
      "data_6/reports_6\n",
      "data_2/reports_2\n",
      "data_7/reports_7\n",
      "data_8/reports_8\n"
     ]
    }
   ],
   "source": [
    "# Need to generate the txt file for train and validation\n",
    "txt_file_path = './dataset/train_tienet.txt'\n",
    "labels_paths = ['../data_4/labels_20k_4.npy',\n",
    "           '../data_5/labels_5.npy', \n",
    "           '../data_6/labels_6.npy',\n",
    "               '../data_2/labels_2.npy',\n",
    "               '../data_7/labels_7.npy',\n",
    "               '../data_8/labels_8.npy']\n",
    "img_dirs = ['../data_4/data/tiffs_20k_4',#]#, \n",
    "           '../data_5/data/tiffs_5', \n",
    "           '../data_6/data/tiffs_6',\n",
    "           '../data_2/data/tiffs_2',\n",
    "           '../data_7/data/tiffs_7',\n",
    "           '../data_8/data/tiffs_8']\n",
    "report_paths = ['../data_4/reports_20k_4',#]#, \n",
    "           '../data_5/reports_5', \n",
    "           '../data_6/reports_6',\n",
    "           '../data_2/reports_2',\n",
    "           '../data_7/reports_7',\n",
    "           '../data_8/reports_8']\n",
    "\n",
    "########################\n",
    "\n",
    "labels = []\n",
    "for labels_path in labels_paths:\n",
    "    labels.append(np.load(labels_path, encoding='bytes'))\n",
    "total_labels = np.concatenate(tuple(labels), axis=0)\n",
    "\n",
    "\n",
    "names_list = []\n",
    "for img_dir in img_dirs:\n",
    "    temp_names_list = [' ']*len(os.listdir(img_dir))\n",
    "    temp_dir_header = img_dir[3:]\n",
    "    print(temp_dir_header)\n",
    "    for fname in os.listdir(img_dir):\n",
    "        # Get actual index \n",
    "        index = int(fname.split('_')[0])\n",
    "        temp_names_list[index] = os.path.join(temp_dir_header, fname)\n",
    "    names_list += temp_names_list\n",
    "    \n",
    "reports_list = []\n",
    "for report_dir in report_paths:\n",
    "    temp_names_list = [' ']*len(os.listdir(report_dir))\n",
    "    temp_dir_header = report_dir[3:]\n",
    "    print(temp_dir_header)\n",
    "    for fname in os.listdir(report_dir):\n",
    "        # Get actual index \n",
    "        index = int(fname.split('_')[0])\n",
    "        temp_names_list[index] = os.path.join(temp_dir_header, fname)\n",
    "    reports_list += temp_names_list\n",
    "\n",
    "d = {'filename':names_list}\n",
    "dtext = {'reportname':reports_list}\n",
    "df1 = pd.DataFrame(data=d)\n",
    "df1 = pd.concat((df1, pd.DataFrame(data=dtext)),axis=1)\n",
    "df2 = pd.DataFrame(data=total_labels)\n",
    "df2 = df2.astype('int')\n",
    "\n",
    "df = pd.concat((df1,df2),axis=1)\n",
    "\n",
    "df.to_csv(txt_file_path, sep=' ', index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_3/data/tiffs_3\n",
      "data_3/reports_20k_3\n"
     ]
    }
   ],
   "source": [
    "# Need to generate the txt file for train and validation\n",
    "txt_file_path = './dataset/valtest_tienet.txt'\n",
    "labels_paths = ['../data_3/labels_20k_3.npy']\n",
    "img_dirs = ['../data_3/data/tiffs_3']\n",
    "report_paths = ['../data_3/reports_20k_3']#, \n",
    "#            '../data_5/reports_5', \n",
    "#            '../data_6/reports_6',\n",
    "#            '../data_2/reports_2',\n",
    "#            '../data_7/reports_7',\n",
    "#            '../data_8/reports_8']\n",
    "\n",
    "########################\n",
    "\n",
    "labels = []\n",
    "for labels_path in labels_paths:\n",
    "    labels.append(np.load(labels_path, encoding='bytes'))\n",
    "total_labels = np.concatenate(tuple(labels), axis=0)\n",
    "\n",
    "\n",
    "names_list = []\n",
    "for img_dir in img_dirs:\n",
    "    temp_names_list = [' ']*len(os.listdir(img_dir))\n",
    "    temp_dir_header = img_dir[3:]\n",
    "    print(temp_dir_header)\n",
    "    for fname in os.listdir(img_dir):\n",
    "        # Get actual index \n",
    "        index = int(fname.split('_')[0])\n",
    "        temp_names_list[index] = os.path.join(temp_dir_header, fname)\n",
    "    names_list += temp_names_list\n",
    "    \n",
    "reports_list = []\n",
    "for report_dir in report_paths:\n",
    "    temp_names_list = [' ']*len(os.listdir(report_dir))\n",
    "    temp_dir_header = report_dir[3:]\n",
    "    print(temp_dir_header)\n",
    "    for fname in os.listdir(report_dir):\n",
    "        # Get actual index \n",
    "        index = int(fname.split('_')[0])\n",
    "        temp_names_list[index] = os.path.join(temp_dir_header, fname)\n",
    "    reports_list += temp_names_list\n",
    "\n",
    "d = {'filename':names_list}\n",
    "dtext = {'reportname':reports_list}\n",
    "df1 = pd.DataFrame(data=d)\n",
    "df1 = pd.concat((df1, pd.DataFrame(data=dtext)),axis=1)\n",
    "df2 = pd.DataFrame(data=total_labels)\n",
    "df2 = df2.astype('int')\n",
    "\n",
    "df = pd.concat((df1,df2),axis=1)\n",
    "\n",
    "df.to_csv(txt_file_path, sep=' ', index = False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert valtest into val and test\n",
    "val_txt_file_path = './dataset/val_tienet.txt'\n",
    "test_txt_file_path = './dataset/test_tienet.txt'\n",
    "index_to_split = 8712\n",
    "\n",
    "df = pd.read_csv(txt_file_path, sep=' ', header=None)\n",
    "df_val = df[:index_to_split]\n",
    "df_test = df[index_to_split:]\n",
    "df_val.to_csv(val_txt_file_path, sep=' ', index=False, header=False)\n",
    "df_test.to_csv(test_txt_file_path, sep=' ', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Text processing from the reports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess the reports \n",
    "report_paths = ['../data_4/reports_20k_4',#]#, \n",
    "           '../data_5/reports_5', \n",
    "           '../data_6/reports_6',\n",
    "           '../data_2/reports_2',\n",
    "           '../data_7/reports_7',\n",
    "           '../data_8/reports_8'] # need to make sure this is the same order as the rpeorts_path above "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bioclean(t):\n",
    "    return re.sub('[.,?;*!%^&_+():-\\[\\]{}]', '', t.replace('\"', '').replace('/', '').replace('\\\\', '').replace(\"'\",'').strip().lower()).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess the reports\n",
    "processed_reports = [] \n",
    "for report_dir in report_paths:\n",
    "    filenames = os.listdir(report_dir)\n",
    "    for filename in filenames:\n",
    "        with open(os.path.join(report_dir, filename),'r') as f:\n",
    "            g = f.read().replace('\\n', '')\n",
    "        f.close()\n",
    "        tokens = bioclean(g)\n",
    "        processed_reports.append(tokens) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For training word2vec model\n",
    "\n",
    "pretrained_word2vec_path = 'pubmed2018_w2v_200D/pubmed2018_w2v_200D.bin'\n",
    "model_path = 'data_456278_pubmed_embeddings'\n",
    "weights_path = 'data_456278_weights.npy'\n",
    "\n",
    "################\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format(pretrained_word2vec_path, binary=True)\n",
    "\n",
    "model = Word2Vec(size=200, min_count=2, workers=16)\n",
    "model.build_vocab(processed_reports)\n",
    "total_examples = model.corpus_count\n",
    "\n",
    "model.build_vocab([list(word_vectors.vocab.keys())], update=True)\n",
    "\n",
    "model.intersect_word2vec_format(pretrained_word2vec_path, binary=True, lockf=1.0)\n",
    "\n",
    "model.train(processed_reports, total_examples=total_examples, epochs=model.epochs)\n",
    "\n",
    "# save the \n",
    "model.wv.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a weight matrix \n",
    "model = KeyedVectors.load(model_path)\n",
    "weights_numpy = np.concatenate((np.zeros((1,*model.vectors.shape[1:])),model.vectors),axis=0) # add 0's for uncommon words \n",
    "weights = torch.FloatTensor(weights_numpy) # formerly syn0, which is soon deprecated\n",
    "\n",
    "# add start and end tokens\n",
    "eps = 1e-3 # the standard deviation of the small random vector we choose for start and end tokens \n",
    "start_token = torch.FloatTensor(np.random.randn(1,weights.shape[1])*eps)\n",
    "end_token = torch.FloatTensor(np.random.randn(1,weights.shape[1])*eps)\n",
    "weights = torch.cat((weights,start_token,end_token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save the weights matrix \n",
    "np.save(weights_path,weights.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15242, 200]) 15239\n"
     ]
    }
   ],
   "source": [
    "print(weights.shape, len(model.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Make thew eight matrix \n",
    "# embed = nn.Embedding(weights.shape[0],weights.shape[1],padding_idx=0)\n",
    "# embed2 = embed.from_pretrained(weights,freeze=False)\n",
    "\n",
    "# emb_layer, _, _ = create_emb_layer(weights, non_trainable=False, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # The pipeline for reading a report and indexing a report\n",
    "# tokens = tokenize_report(os.path.join(report_dir, filenames[0]))\n",
    "# report = index_report(tokens, model, weights.shape[0], include_start_end=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the TieNet model\n",
    "\n",
    "<a id='train2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def create_emb_layer(weights_matrix, non_trainable=False, padding_idx=0):\n",
    "#     num_embeddings, embedding_dim = weights_matrix.size()\n",
    "#     emb_layer = nn.Embedding(num_embeddings, embedding_dim, padding_idx=padding_idx)\n",
    "#     emb_layer.load_state_dict({'weight': weights_matrix})\n",
    "#     if non_trainable:\n",
    "#         emb_layer.weight.requires_grad = False\n",
    "\n",
    "#     return emb_layer\n",
    "\n",
    "\n",
    "# class ResNet50(nn.Module):\n",
    "#     def __init__(self, classCount, isTrained, trainable=False):\n",
    "    \n",
    "#         super(ResNet50, self).__init__()\n",
    "        \n",
    "#         self.resnet50 = torchvision.models.resnet50(pretrained=isTrained)\n",
    "#         # Take out the global average pooling and fully connected layers \n",
    "#         self.features = nn.Sequential(*list(self.resnet50.children())[:-2])\n",
    "#         for child in self.features.children():\n",
    "#             for param in child.parameters():\n",
    "#                 if trainable:\n",
    "#                     param.requires_grad = True\n",
    "#                 else:\n",
    "#                     param.requires_grad = False \n",
    "#     def forward(self, x):\n",
    "#         x = self.features(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "                \n",
    "# class ImageEncoder(nn.Module):\n",
    "#     # Assumes use of 224*224 images \n",
    "#     def __init__(self, transition_dim, lstm_hidden_dim):\n",
    "#         super(ImageEncoder, self).__init__()\n",
    "#         #\n",
    "#         self.cnn = ResNet50(14, True, trainable=False)\n",
    "#         self.conv1 = nn.Conv2d(2048, transition_dim, 1)  \n",
    "#         #\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         x = self.cnn(x) # Initial feature extraction \n",
    "#         x = self.conv1(x) # transition conv layer\n",
    "#         x = F.relu(x) \n",
    "#         # need to get the activity a0 to elementwise multiply with X \n",
    "#         return x\n",
    "\n",
    "# class AttentionLSTMDecoder(nn.Module):\n",
    "#     # LSTM decoder component of TieNet \n",
    "#     def __init__(self, transition_dim, lstm_hidden_dim, embedding_dim, output_dim, weights):\n",
    "#         self.embedding_dim = embedding_dim\n",
    "#         self.hidden_dim = lstm_hidden_dim\n",
    "#         self.output_dim = output_dim\n",
    "#         self.encode_output_dim = encode_output_dim\n",
    "#         self.weights = weights\n",
    "        \n",
    "#         self.transition = nn.Linear(transition_dim*7*7, lstm_hidden_dim)\n",
    "#         self.embed = create_embed_layer(weights, non_trainable=False, padding_idx=0)\n",
    "        \n",
    "#     def forward(self, word, hidden, encoded):\n",
    "#         hidden = self.transition(encoded.view(encoded.size(0),-1))\n",
    "#         word_embedded = self.embed(word)\n",
    "    \n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, transition_dim=1024):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        resnet = torchvision.models.resnet50(pretrained=True)  # pretrained ImageNet ResNet-101\n",
    "\n",
    "        # Remove linear and pool layers (since we're not doing classification)\n",
    "        modules = list(resnet.children())[:-2]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.fine_tune(fine_tune=False) # don't make any layers of ResNet trainable \n",
    "        \n",
    "        ## Resize image to fixed size to allow input images of variable size\n",
    "        #self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
    "        \n",
    "        self.transition_conv = nn.Conv2d(2048, transition_dim, 1) # output shape (1024, 7, 7)\n",
    "#         self.transition_conv = nn.ConvTranspose2d(2048, transition_dim, 10) # output shape (1024, 16, 16) as in the paper\n",
    "        \n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
    "        :return: encoded images\n",
    "        \"\"\"\n",
    "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
    "#         out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
    "#         out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
    "\n",
    "        out = self.transition_conv(out) \n",
    "        out = F.relu(out) \n",
    "        out = out.permute(0,2,3,1) # (batch_size, encoded_image_size, encoded_image_size, 1024) \n",
    "        \n",
    "        return out\n",
    "\n",
    "    def fine_tune(self, fine_tune=False):\n",
    "        \"\"\"\n",
    "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
    "        :param fine_tune: Allow?\n",
    "        \"\"\"\n",
    "        for p in self.resnet.parameters():\n",
    "            p.requires_grad = False\n",
    "        # If fine-tuning, only fine-tune convolutional blocks 2 through 4\n",
    "        for c in list(self.resnet.children())[5:]:\n",
    "            for p in c.parameters():\n",
    "                p.requires_grad = fine_tune\n",
    "\n",
    "\n",
    "    \n",
    "class Attention(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Network.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
    "        \"\"\"\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param decoder_dim: size of decoder's RNN\n",
    "        :param attention_dim: size of the attention network\n",
    "        \"\"\"\n",
    "        super(Attention, self).__init__()\n",
    "        self.encoder_att = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
    "        self.decoder_att = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
    "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
    "\n",
    "    def forward(self, encoder_out, decoder_hidden):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
    "        :return: attention weighted encoding, weights\n",
    "        \"\"\"\n",
    "        att1 = self.encoder_att(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
    "        att2 = self.decoder_att(decoder_hidden)  # (batch_size, attention_dim)\n",
    "        att = self.full_att(self.relu(att1 + att2.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
    "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
    "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, num_pixels, encoder_dim)\n",
    "\n",
    "        return attention_weighted_encoding, alpha\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecoderWithAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Decoder. Also includes Saliency-weighted global average pooling and attention encoded text embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, \n",
    "                 num_classes=14, aete_num_global_attentions=5, aete_dimensionality=2000, \n",
    "                 encoder_dim=1024, dropout=0.5):\n",
    "        \"\"\"\n",
    "        :param attention_dim: size of attention network\n",
    "        :param embed_dim: embedding size\n",
    "        :param decoder_dim: size of decoder's RNN \n",
    "        :param vocab_size: size of vocabulary\n",
    "        :param encoder_dim: feature size of encoded images\n",
    "        :param dropout: dropout\n",
    "        :num_classes: the number of classes for multilabel training (default is 14 for Chest X rays)\n",
    "        :num_global_attentions: r parameter (TieNet paper uses 5)\n",
    "        :dimensionality: s parameter (TieNet paper uses 2000)\n",
    "        \"\"\"\n",
    "        super(DecoderWithAttention, self).__init__()\n",
    "\n",
    "        self.encoder_dim = encoder_dim\n",
    "        self.attention_dim = attention_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.decoder_dim = decoder_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        \n",
    "        self.aete = AETE(decoder_dim, aete_num_global_attentions, aete_dimensionality) # attention encoded text embedding\n",
    "        self.swgap = SaliencyGlobalAvgPool(aete_num_global_attentions) # saliency-weighted global avg pool\n",
    "        #self.poolt = nn.MaxPool1d(aete_num_global_attentions)\n",
    "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)  # embedding layer\n",
    "        self.dropout = nn.Dropout(p=self.dropout)\n",
    "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
    "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
    "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
    "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
    "        self.fc_final = nn.Linear(decoder_dim+encoder_dim, num_classes)\n",
    "        self.init_weights()  # initialize some layers with the uniform distribution\n",
    "\n",
    "    def init_weights(self):\n",
    "        \"\"\"\n",
    "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
    "        \"\"\"\n",
    "        #self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def load_pretrained_embeddings(self, embeddings):\n",
    "        \"\"\"\n",
    "        Loads embedding layer with pre-trained embeddings.\n",
    "        :param embeddings: pre-trained embeddings\n",
    "        \"\"\"\n",
    "        self.embedding.weight = nn.Parameter(embeddings)\n",
    "        #self.embedding.load_state_dict({'weight': embeddings})\n",
    "\n",
    "    def fine_tune_embeddings(self, fine_tune=True):\n",
    "        \"\"\"\n",
    "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
    "        :param fine_tune: Allow?\n",
    "        \"\"\"\n",
    "        for p in self.embedding.parameters():\n",
    "            p.requires_grad = fine_tune\n",
    "\n",
    "    def init_hidden_state(self, encoder_out):\n",
    "        \"\"\"\n",
    "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
    "        :return: hidden state, cell state\n",
    "        \"\"\"\n",
    "        mean_encoder_out = encoder_out.mean(dim=1)\n",
    "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
    "        c = self.init_c(mean_encoder_out)\n",
    "        return h, c\n",
    "\n",
    "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
    "        \"\"\"\n",
    "        Forward propagation.\n",
    "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
    "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
    "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
    "        :return: scores for vocabulary, decode lengths, sort indices, classification output\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size = encoder_out.size(0)\n",
    "        encoder_dim = encoder_out.size(-1)\n",
    "        vocab_size = self.vocab_size\n",
    "\n",
    "        # Flatten image\n",
    "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
    "        num_pixels = encoder_out.size(1)\n",
    "\n",
    "        # Sort input data by decreasing lengths; why? apparent below\n",
    "        caption_lengths, sort_ind = caption_lengths.sort(dim=0, descending=True)\n",
    "        encoder_out = encoder_out[sort_ind]\n",
    "        encoded_captions = encoded_captions[sort_ind]\n",
    "\n",
    "        # Embedding\n",
    "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
    "\n",
    "        # Initialize LSTM state\n",
    "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
    "\n",
    "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
    "        # So, decoding lengths are actual lengths - 1\n",
    "        decode_lengths = (caption_lengths - 1).tolist()\n",
    "\n",
    "        # Create tensors to hold word predicion scores and alphas\n",
    "        predictions = torch.zeros(batch_size, max(decode_lengths), vocab_size).to(device)\n",
    "        alphas = torch.zeros(batch_size, max(decode_lengths), num_pixels).to(device)\n",
    "        hiddens = torch.zeros(batch_size, max(decode_lengths), h.size(-1)).to(device)\n",
    "\n",
    "        # At each time-step, decode by\n",
    "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
    "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
    "        for t in range(max(decode_lengths)):\n",
    "            batch_size_t = sum([l > t for l in decode_lengths])\n",
    "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
    "                                                                h[:batch_size_t])\n",
    "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
    "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
    "            h, c = self.decode_step(\n",
    "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
    "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
    "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
    "            predictions[:batch_size_t, t, :] = preds\n",
    "            alphas[:batch_size_t, t, :] = alpha\n",
    "            hiddens[:batch_size_t, t, :] = h\n",
    "            \n",
    "        # Attention encoded text embedding and saliency-weighted global average pooling \n",
    "        x_aete, g = self.aete(hiddens, decode_lengths)\n",
    "        x_swgap = self.swgap(g, alphas, encoder_out)\n",
    "    \n",
    "    \n",
    "        # concatenate these two \n",
    "        x = torch.cat([x_aete, x_swgap], dim=-1)\n",
    "        output = self.fc_final(x)\n",
    "        \n",
    "        return predictions, decode_lengths, sort_ind, output, encoded_captions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now make the Attention encoded text embedding part of the network\n",
    "\n",
    "class AETE(nn.Module):\n",
    "    \"\"\"\n",
    "    Attention Encoded Text Embedding.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, decoder_dim, num_global_attentions, dimensionality):\n",
    "        # num_global_attentions refers to \"r\" parameter in paper, set to 5\n",
    "        # dimensionality refers to \"s\" parameter in paper, set to 2000\n",
    "        super(AETE, self).__init__()\n",
    "        self.Ws2 = nn.Linear(dimensionality, num_global_attentions)\n",
    "        self.Ws1 = nn.Linear(decoder_dim, dimensionality)\n",
    "        self.tanh = nn.Tanh() \n",
    "        self.softmax = nn.Softmax(dim=1) \n",
    "        self.pool = nn.MaxPool1d(num_global_attentions)\n",
    "        self.num_global_attentions = num_global_attentions\n",
    "    \n",
    "    def forward(self, hidden_states, decode_lengths):\n",
    "        # hidden_states contains the whole sequence (in time) of hidden states (batch_size, caption_lengths, decoder_dim)\n",
    "        batch_size = hidden_states.size(0)\n",
    "        decoder_dim = hidden_states.size(-1)\n",
    "        \n",
    "        ms = torch.zeros(batch_size, decoder_dim).to(device)\n",
    "        gs = torch.zeros(batch_size, max(decode_lengths), self.num_global_attentions).to(device)\n",
    "        for ind, t in enumerate(decode_lengths): \n",
    "            hidden_state = hidden_states[ind]\n",
    "            hidden_state = hidden_state[:t]\n",
    "            g = self.Ws1(hidden_state) # shape (caption_lengths, dimensionality)\n",
    "            g = self.tanh(g) \n",
    "            g = self.Ws2(g) # (caption_lengths, num_global_attentions)\n",
    "            g = self.softmax(g)\n",
    "            m = torch.matmul(hidden_state.t(), g) # (decoder_dim, num_global_attentions)\n",
    "            #print(m.shape)\n",
    "            m = self.pool(m.unsqueeze(0)).squeeze() # (decoder_dim,)\n",
    "            ms[ind] = m # this is the X_aete that we use for joint learning \n",
    "            gs[ind,:t] = g\n",
    "            \n",
    "        return ms, gs\n",
    "    \n",
    "# We make the saliency weighted global average pooling\n",
    "class SaliencyGlobalAvgPool(nn.Module):\n",
    "    \"\"\"\n",
    "    Saliency Weighted Global Average Pooling\n",
    "    \"\"\"\n",
    "    def __init__(self, num_global_attentions):\n",
    "        super(SaliencyGlobalAvgPool, self).__init__()\n",
    "        self.pool = nn.MaxPool1d(num_global_attentions)\n",
    "        \n",
    "    def forward(self, g, alpha, encoder_output):\n",
    "        g = self.pool(g) # g input is (batch_size, decoder_length, num_global_attentions) --. (batch_size,decoder_length,1)\n",
    "        # Multiply these vectors by the image attention \n",
    "        # size alpha = (batch_size, decoder_length, num_pixels)\n",
    "        aws = g*alpha # (batch_size, decoder_length, num_pixels)\n",
    "        #aws = torch.mul(g, alpha)\n",
    "        aws = aws.sum(dim=1) # (batch_size, num_pixels)\n",
    "        # Now multiply by the encoded image \n",
    "        x_swgap = aws.unsqueeze(2) * encoder_output # (batch_size, num_pixels, encoder_dim)\n",
    "        x_swgap = x_swgap.sum(dim=1) # (batch_size, encoder_dim)\n",
    "        return x_swgap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Performs one epoch's training.\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param criterion: loss layer\n",
    "    :param encoder_optimizer: optimizer to update encoder's weights (if fine-tuning)\n",
    "    :param decoder_optimizer: optimizer to update decoder's weights\n",
    "    :param epoch: epoch number\n",
    "    \"\"\"\n",
    "\n",
    "    decoder.train()  # train mode (dropout and batchnorm is used)\n",
    "    encoder.train()\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss (per word decoded)\n",
    "    auroc_avg = AverageMeter() # auroc per sample \n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Batches\n",
    "    for i, (imgs, text, textlens, label) in tqdm(enumerate(train_loader)):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        # Move to GPU, if available\n",
    "        imgs = imgs.cuda(async=True)\n",
    "        text = text.cuda(async=True)\n",
    "        textlens = textlens.cuda(async=True)\n",
    "        label = label.cuda(async=True)\n",
    "\n",
    "        # Forward prop.\n",
    "        imgs = encoder(imgs) # (batch_size, img_dim, img_dim, encoder_dim)\n",
    "        \n",
    "        # scores = word predictions \n",
    "        word_predictions, decode_lengths, sort_ind, output, text_sorted= decoder(imgs, text, textlens)\n",
    "        label = label[sort_ind] # since they were re-sorted\n",
    "        \n",
    "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
    "        targets = text_sorted[:, 1:] # this is the target words that we are trying to predict \n",
    "        # Remove timesteps that we didn't decode at, or are pads\n",
    "        # pack_padded_sequence is an easy trick to do this\n",
    "        scores, _ = pack_padded_sequence(word_predictions, decode_lengths, batch_first=True)\n",
    "        targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(output, label, scores, targets)\n",
    "\n",
    "        # Back prop.\n",
    "        decoder_optimizer.zero_grad()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(decoder_optimizer, grad_clip)\n",
    "            if encoder_optimizer is not None:\n",
    "                clip_gradient(encoder_optimizer, grad_clip)\n",
    "\n",
    "        # Update weights\n",
    "        decoder_optimizer.step()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.step()\n",
    "\n",
    "        # Keep track of metrics\n",
    "#         auroc = computeAUROC(label, output, 14)[-1] # get the overall AUROC of batch \n",
    "        losses.update(loss.item(), len(decode_lengths))\n",
    "        batch_time.update(time.time() - start)\n",
    "#         auroc_avg.update(auroc, len(decode_lengths))\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(epoch, i, len(train_loader),\n",
    "                                                                          batch_time=batch_time,\n",
    "                                                                          data_time=data_time, loss=losses))\n",
    "                                                                          #auroc=auroc, auroc_avg=auroc_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(val_loader, encoder, decoder, criterion):\n",
    "    \"\"\"\n",
    "    Performs one epoch's validation.\n",
    "    :param val_loader: DataLoader for validation data.\n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param criterion: loss layer\n",
    "    :return: roc_auc score \n",
    "    \"\"\"\n",
    "    decoder.eval()  # eval mode (no dropout or batchnorm)\n",
    "    if encoder is not None:\n",
    "        encoder.eval()\n",
    "\n",
    "#     batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "#     aucs = AverageMeter()\n",
    "\n",
    "#     start = time.time()\n",
    "\n",
    "\n",
    "    # explicitly disable gradient calculation to avoid CUDA memory error\n",
    "    # solves the issue #57\n",
    "    \n",
    "    labels = torch.FloatTensor().cuda()\n",
    "    outputs = torch.FloatTensor().cuda()\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i, (imgs, text, textlens, label) in enumerate(val_loader):\n",
    "\n",
    "            # Move to device, if available\n",
    "            imgs = imgs.to(device)\n",
    "            text = text.to(device)\n",
    "            textlens = textlens.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Forward prop.\n",
    "            if encoder is not None:\n",
    "                imgs = encoder(imgs)\n",
    "            word_predictions, decode_lengths, sort_ind, output, text_sorted = decoder(imgs, text, textlens)\n",
    "            label = label[sort_ind]\n",
    "\n",
    "            # Calculate loss\n",
    "            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
    "            targets = text_sorted[:, 1:] # this is the target words that we are trying to predict \n",
    "            # Remove timesteps that we didn't decode at, or are pads\n",
    "            # pack_padded_sequence is an easy trick to do this\n",
    "            scores, _ = pack_padded_sequence(word_predictions, decode_lengths, batch_first=True)\n",
    "            targets, _ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "            loss = criterion(output, label, scores, targets)\n",
    "            outputs = torch.cat((outputs, output), 0)\n",
    "            labels = torch.cat((labels, label), 0)\n",
    "\n",
    "            # Keep track of metrics\n",
    "            losses.update(loss.item(), len(decode_lengths))\n",
    "            \n",
    "    aucs = computeAUROC(labels, outputs, 14)\n",
    "    print(\"Validation AUC:\", aucs[-1])\n",
    "    print(\"Specific AUCs:\", aucs[:-1])\n",
    "    print(\"Validation Loss:\", losses.avg)\n",
    "    return aucs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clip_gradient(optimizer, grad_clip):\n",
    "    \"\"\"\n",
    "    Clips gradients computed during backpropagation to avoid explosion of gradients.\n",
    "    :param optimizer: optimizer with the gradients to be clipped\n",
    "    :param grad_clip: clip value\n",
    "    \"\"\"\n",
    "    for group in optimizer.param_groups:\n",
    "        for param in group['params']:\n",
    "            if param.grad is not None:\n",
    "                param.grad.data.clamp_(-grad_clip, grad_clip)\n",
    "                \n",
    "def adjust_learning_rate(optimizer, shrink_factor):\n",
    "    \"\"\"\n",
    "    Shrinks learning rate by a specified factor.\n",
    "    :param optimizer: optimizer whose learning rate must be shrunk.\n",
    "    :param shrink_factor: factor in interval (0, 1) to multiply learning rate with.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nDECAYING learning rate.\")\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = param_group['lr'] * shrink_factor\n",
    "    print(\"The new learning rate is %f\\n\" % (optimizer.param_groups[0]['lr'],))\n",
    "    \n",
    "    \n",
    "def save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "                    overall_auroc, class_aurocs, is_best):\n",
    "    \"\"\"\n",
    "    Saves model checkpoint.\n",
    "    :param data_name: base name of processed dataset\n",
    "    :param epoch: epoch number\n",
    "    :param epochs_since_improvement: number of epochs since last improvement in BLEU-4 score\n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param encoder_optimizer: optimizer to update encoder's weights, if fine-tuning\n",
    "    :param decoder_optimizer: optimizer to update decoder's weights\n",
    "    :param bleu4: validation auroc\n",
    "    :param is_best: is this checkpoint the best so far?\n",
    "    \"\"\"\n",
    "    state = {'epoch': epoch,\n",
    "             'epochs_since_improvement': epochs_since_improvement,\n",
    "             'overall_auroc': overall_auroc,\n",
    "             'class_aurocs': class_aurocs, \n",
    "             'encoder': encoder.state_dict(),\n",
    "             'decoder': decoder.state_dict(),\n",
    "             'encoder_optimizer': encoder_optimizer.state_dict(),\n",
    "             'decoder_optimizer': decoder_optimizer.state_dict()}\n",
    "    filename = 'checkpoint_' + data_name + '.pth.tar'\n",
    "    torch.save(state, 'models/'+filename)\n",
    "    # If this checkpoint is the best so far, store a copy so it doesn't get overwritten by a worse checkpoint\n",
    "    if is_best:\n",
    "        torch.save(state, 'models/BEST_' + filename)\n",
    "        \n",
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Keeps track of most recent, average, sum, and count of a metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "        \n",
    "def computeAUROC (dataGT, dataPRED, classCount):\n",
    "        \n",
    "        outAUROC = []\n",
    "        \n",
    "        datanpGT = dataGT.cpu().detach().numpy()\n",
    "        datanpPRED = dataPRED.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "        total_num_samples = [] \n",
    "        for i in range(classCount):\n",
    "            datanpGT_temp = datanpGT[:,i] \n",
    "            datanpGT_nonneg = datanpGT_temp[datanpGT_temp >= 0]\n",
    "            datanpPRED_nonneg = datanpPRED[:,i]\n",
    "            datanpPRED_nonneg = datanpPRED_nonneg[datanpGT_temp >= 0]\n",
    "            num_samples = len(datanpGT_nonneg)\n",
    "            outAUROC.append(roc_auc_score(datanpGT_nonneg, datanpPRED_nonneg))\n",
    "            total_num_samples.append(num_samples)\n",
    "            #print(outAUROC[i], num_samples)\n",
    "\n",
    "        overallAUROC = np.sum(np.array(outAUROC)*np.array(total_num_samples))/np.sum(total_num_samples)\n",
    "\n",
    "        outAUROC.append(overallAUROC)\n",
    "        \n",
    "        return outAUROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actually train \n",
    "<a id='train'></a>\n",
    "\n",
    "[Click here to go to evaluation code](#eval)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.67177793 0.9671413  0.79564277 0.76717925 0.97223951 0.87407297\n",
      " 0.95112196 0.91820742 0.78678206 0.95289435 0.75602778 0.99179818\n",
      " 0.98021823 0.7062537 ] 0.13653332124909798 [6.324219398248958]\n"
     ]
    }
   ],
   "source": [
    "# First, for the loss function we want to weight the classes so we first compute those parameters\n",
    "# First compute the class weights for the training data set \n",
    "pathFileTrain = 'dataset/train_tienet.txt'\n",
    "actual_labels = pd.read_csv(pathFileTrain,delimiter=' ',header=None)\n",
    "#actual_labels.head()\n",
    "act = actual_labels[actual_labels.columns[2:]]\n",
    "actarray = np.array(act)\n",
    "\n",
    "lambda_m = np.array([(len(arr[arr>=0])-arr[arr>=0].sum())/len(arr[arr>=0]) for arr in actarray.transpose()]) \n",
    "beta_N = actarray[actarray >= 0].sum()/len(actarray[actarray >= 0])\n",
    "beta_P = [(1-beta_N)/beta_N]\n",
    "\n",
    "print(lambda_m, beta_N, beta_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c40b0ce7972643a19715a844a839b52d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/3331]\tBatch Time 2.316 (2.316)\tData Load Time 1.269 (1.269)\tLoss 7.4572 (7.4572)\t\n",
      "Epoch: [0][100/3331]\tBatch Time 0.770 (0.894)\tData Load Time 0.002 (0.014)\tLoss 3.1048 (4.7067)\t\n",
      "Epoch: [0][200/3331]\tBatch Time 0.902 (0.889)\tData Load Time 0.002 (0.008)\tLoss 2.5444 (3.7761)\t\n",
      "Epoch: [0][300/3331]\tBatch Time 0.814 (0.868)\tData Load Time 0.002 (0.006)\tLoss 2.5773 (3.3552)\t\n",
      "Epoch: [0][400/3331]\tBatch Time 0.850 (0.865)\tData Load Time 0.002 (0.005)\tLoss 2.3226 (3.1081)\t\n",
      "Epoch: [0][500/3331]\tBatch Time 0.853 (0.860)\tData Load Time 0.001 (0.004)\tLoss 2.2706 (2.9430)\t\n",
      "Epoch: [0][600/3331]\tBatch Time 0.865 (0.853)\tData Load Time 0.001 (0.004)\tLoss 2.2252 (2.8177)\t\n",
      "Epoch: [0][700/3331]\tBatch Time 0.868 (0.844)\tData Load Time 0.001 (0.004)\tLoss 2.2099 (2.7191)\t\n",
      "Epoch: [0][800/3331]\tBatch Time 0.790 (0.843)\tData Load Time 0.002 (0.003)\tLoss 2.0843 (2.6426)\t\n",
      "Epoch: [0][900/3331]\tBatch Time 0.741 (0.839)\tData Load Time 0.001 (0.003)\tLoss 1.8125 (2.5768)\t\n",
      "Epoch: [0][1000/3331]\tBatch Time 0.670 (0.837)\tData Load Time 0.002 (0.003)\tLoss 2.0224 (2.5216)\t\n",
      "Epoch: [0][1100/3331]\tBatch Time 1.806 (0.839)\tData Load Time 0.002 (0.003)\tLoss 2.0074 (2.4741)\t\n"
     ]
    }
   ],
   "source": [
    "# Code for training\n",
    "\n",
    "# Parameters\n",
    "# Data parameters\n",
    "data_name = 'tienet20k_dualloss_alldata'# for checkpoint \n",
    "train_report_dirs = ['../data_4/reports_20k_4',#]#, \n",
    "           '../data_5/reports_5', \n",
    "           '../data_6/reports_6',\n",
    "           '../data_2/reports_2',\n",
    "           '../data_7/reports_7',\n",
    "           '../data_8/reports_8']\n",
    "val_report_dir = '../data_3/reports_20k_3'\n",
    "pathDirDataTrain = '..'\n",
    "pathDirDataVal = '..' #None # if none, don't use validation \n",
    "pathFileTrain = 'dataset/train_tienet.txt'\n",
    "pathFileVal = 'dataset/val_tienet.txt'\n",
    "embedding_weight_path = 'data4_weights.npy'\n",
    "embeddingModelFile = 'data_4_pubmed_embeddings'\n",
    "\n",
    "# Preprocessing\n",
    "transCrop = 224 # size of the final image\n",
    "longest_train = 539 # if we know the longest sequence (remember to add 2 for the start/end tokens)\n",
    "longest_val = 348\n",
    "\n",
    "# Model parameters\n",
    "emb_dim = 200  # dimension of word embeddings\n",
    "vocab_size = 15242 # number of words in vocabulary, including <start>, <end>, and pad index/unknown words (0)\n",
    "encoder_dim = 1024 # dimension of the encoded images \n",
    "attention_dim = 1024  # dimension of attention linear layers, originally 350\n",
    "decoder_dim = 256 # dimension of decoder RNN\n",
    "aete_num_global_attentions = 5 # for attention encoded text embedding\n",
    "aete_dimensionality = 2000 # for attention encoded text embedding\n",
    "num_classes = 14 # number of classes for classification\n",
    "dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = False  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "# Training parameters\n",
    "epochs = 100  # number of epochs to train for (if early stopping is not triggered)\n",
    "patience = 10 # number of epochs to wait for improvement before stopping \n",
    "batch_size = 32\n",
    "alpha = 0.3 # how much to wait the classification loss vs. the RNN loss \n",
    "num_workers = 12  # for data-loading\n",
    "encoder_lr = 1e-3  # learning rate for encoder if fine-tuning\n",
    "decoder_lr = 1e-3  # learning rate for decoder\n",
    "grad_clip = 5.  # clip gradients at an absolute value of\n",
    "print_freq = 100  # print training/validation stats every __ batches\n",
    "fine_tune_encoder = False  # fine-tune encoder? No \n",
    "checkpoint = None# 'models/checkpoint_'+data_name+'.pth.tar'  # path to checkpoint, None if none\n",
    "\n",
    "###############################################################################\n",
    "## Actual model\n",
    "\n",
    "\n",
    "\n",
    "if checkpoint is None:\n",
    "    decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
    "                                   embed_dim=emb_dim,\n",
    "                                   decoder_dim=decoder_dim,\n",
    "                                   vocab_size=vocab_size,\n",
    "                                   num_classes=num_classes, \n",
    "                                   aete_num_global_attentions=aete_num_global_attentions, \n",
    "                                   aete_dimensionality=aete_dimensionality,\n",
    "                                   encoder_dim=encoder_dim,\n",
    "                                   dropout=dropout)\n",
    "    if embedding_weight_path is not None:\n",
    "        embedding_weights = torch.Tensor(np.load(embedding_weight_path)).cuda()\n",
    "        decoder.load_pretrained_embeddings(embedding_weights)\n",
    "        decoder.fine_tune_embeddings(fine_tune=True)\n",
    "        \n",
    "    decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
    "                                         lr=decoder_lr)\n",
    "    encoder = Encoder(encoder_dim)\n",
    "    encoder.fine_tune(fine_tune_encoder) # this refers only to the resnet part and does not refer to the transition layer \n",
    "    encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),lr=encoder_lr) #if fine_tune_encoder else None\n",
    "    start_epoch = 0\n",
    "    epochs_since_improvement = 0\n",
    "    best_auroc = 0\n",
    "    decoder = decoder.cuda()\n",
    "    encoder = encoder.cuda()\n",
    "else:\n",
    "    checkpoint = torch.load(checkpoint)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
    "    best_auroc = checkpoint['overall_auroc']\n",
    "    \n",
    "    decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
    "                                   embed_dim=emb_dim,\n",
    "                                   decoder_dim=decoder_dim,\n",
    "                                   vocab_size=vocab_size,\n",
    "                                   num_classes=num_classes, \n",
    "                                   aete_num_global_attentions=aete_num_global_attentions, \n",
    "                                   aete_dimensionality=aete_dimensionality,\n",
    "                                   encoder_dim=encoder_dim,\n",
    "                                   dropout=dropout)\n",
    "    encoder = Encoder(encoder_dim)\n",
    "    \n",
    "#     decoder = checkpoint['decoder']\n",
    "    decoder.load_state_dict(checkpoint['decoder'])\n",
    "    encoder.load_state_dict(checkpoint['encoder'])\n",
    "    \n",
    "    decoder = decoder.cuda()\n",
    "    encoder = encoder.cuda()\n",
    "    \n",
    "    decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
    "                                         lr=decoder_lr)\n",
    "    encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                         lr=encoder_lr)\n",
    "    decoder_optimizer.load_state_dict(checkpoint['decoder_optimizer'])\n",
    "    encoder_optimizer.load_state_dict(checkpoint['encoder_optimizer'])\n",
    "    if fine_tune_encoder is True and encoder_optimizer is None:\n",
    "        encoder.fine_tune(fine_tune_encoder)\n",
    "        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                             lr=encoder_lr)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Normalization and preprocessing the images \n",
    "normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imageNet parameters     \n",
    "transformList = []\n",
    "transformList.append(transforms.RandomResizedCrop(transCrop))\n",
    "transformList.append(transforms.RandomHorizontalFlip())\n",
    "transformList.append(transforms.ToTensor())\n",
    "transformList.append(normalize)      \n",
    "transformSequence=transforms.Compose(transformList)\n",
    "\n",
    "\n",
    "# Dataset loader \n",
    "\n",
    "# We might need to get the max sequence length in each dataset:\n",
    "if longest_train is None:\n",
    "    max_train = 0 \n",
    "    for train_report_dir in train_report_dirs:\n",
    "        all_reports_train = preprocess_all_reports(train_report_dir)\n",
    "        longest_train = len(max(all_reports_train, key=len))+2 # add 2 for start/end tokens \n",
    "        if longest_train > max_train:\n",
    "            max_train = longest_train \n",
    "        print(\"Longest report (train) in %s:\"%train_report_dir, longest_train)\n",
    "    longest_train = max_train \n",
    "if longest_val is None:\n",
    "    all_reports_val = preprocess_all_reports(val_report_dir)\n",
    "    longest_val = len(max(all_reports_val, key=len))+2 # add 2 for start/end tokens \n",
    "    print(\"Longest report (val): \", longest_val)\n",
    "    \n",
    "datasetTrain = TienetTrainDatasetGenerator(pathImageDirectory=pathDirDataTrain, \n",
    "                                           pathReportDirectory=pathDirDataTrain,\n",
    "                                           pathDatasetFile=pathFileTrain,\n",
    "                                           embeddingModelFile=embeddingModelFile,\n",
    "                                           transform=transformSequence, \n",
    "                                           maxReportLength=longest_train)\n",
    "train_loader = DataLoader(dataset=datasetTrain, batch_size=batch_size, shuffle=True,  \n",
    "                             num_workers=num_workers, pin_memory=True)\n",
    "if pathDirDataVal != None:\n",
    "    datasetVal =   TienetTrainDatasetGenerator(pathImageDirectory=pathDirDataVal, \n",
    "                                               pathReportDirectory=pathDirDataVal, \n",
    "                                               pathDatasetFile=pathFileVal, \n",
    "                                               embeddingModelFile=embeddingModelFile,\n",
    "                                               transform=transformSequence, \n",
    "                                               maxReportLength=longest_val)\n",
    "    val_loader = DataLoader(dataset=datasetVal, batch_size=batch_size, shuffle=False, \n",
    "                               num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    \n",
    "# Loss, use masked_BCE\n",
    "criterion = TieNetLoss(alpha=alpha, beta_P=beta_P, beta_N=beta_N, lambda_m=lambda_m)\n",
    "\n",
    "\n",
    "# Epochs\n",
    "for epoch in range(start_epoch, epochs):\n",
    "\n",
    "    # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
    "    if epochs_since_improvement == patience:\n",
    "        break\n",
    "    ## The below is only if we want to adjust the learning rate, but the paper just keeps a constant learning rate\n",
    "#     if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
    "#         adjust_learning_rate(decoder_optimizer, 0.8)\n",
    "#         if fine_tune_encoder:\n",
    "#             adjust_learning_rate(encoder_optimizer, 0.8)\n",
    "\n",
    "    # One epoch's training\n",
    "    train(train_loader=train_loader,\n",
    "          encoder=encoder,\n",
    "          decoder=decoder,\n",
    "          criterion=criterion,\n",
    "          encoder_optimizer=encoder_optimizer,\n",
    "          decoder_optimizer=decoder_optimizer,\n",
    "          epoch=epoch)\n",
    "\n",
    "    # One epoch's validation\n",
    "    if pathDirDataVal != None:\n",
    "        recent_aurocs  = validate(val_loader=val_loader,\n",
    "                            encoder=encoder,\n",
    "                            decoder=decoder,\n",
    "                            criterion=criterion)\n",
    "        recent_auroc = recent_aurocs[-1] # the last AUC value \n",
    "        class_aurocs = recent_aurocs[:-1]\n",
    "\n",
    "        # Check if there was an improvement\n",
    "        is_best = recent_auroc > best_auroc\n",
    "        best_auroc = max(recent_auroc, best_auroc)\n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
    "        else:\n",
    "            epochs_since_improvement = 0\n",
    "    else:\n",
    "        is_best = False\n",
    "        class_aurocs = 0 \n",
    "        recent_auroc = 0 \n",
    "        \n",
    "    # Save checkpoint\n",
    "    save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n",
    "                    decoder_optimizer, recent_auroc, class_aurocs, is_best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Jump back to beginning](#start)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "<a id='eval'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87b1140d06494aefb1266391e586850e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At index %d, the overall AUC is: 0.5758053285875293\n",
      "At index %d, the overall AUC is: 0.5712111498891302\n",
      "At index %d, the overall AUC is: 0.5669990281439589\n",
      "At index %d, the overall AUC is: 0.5700232250227784\n",
      "At index %d, the overall AUC is: 0.5736844809961921\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pathModel= 'models/checkpoint_tienet20k_dualloss.pth.tar'\n",
    "pathDirDataTest = '..'\n",
    "pathFileTest = 'dataset/test_tienet.txt'\n",
    "embeddingModelFile = 'data_4_pubmed_embeddings'\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "report_exists = True # if it does, then the pathFileTest txt file needs to be adjusted accordingly to extract label \n",
    "img_size = 224 # size of the final image\n",
    "start_token_index = 7708 # the index for the start token \n",
    "end_token_index = 7709 \n",
    "max_report_length = 300 # maximum number of words to be sampled for a given report \n",
    "\n",
    "# Model parameters\n",
    "emb_dim = 200  # dimension of word embeddings\n",
    "vocab_size = 7710 # number of words in vocabulary, including <start>, <end>, and pad index/unknown words (0)\n",
    "encoder_dim = 1024 # dimension of the encoded images \n",
    "attention_dim = 350  # dimension of attention linear layers\n",
    "decoder_dim = 256 # dimension of decoder RNN\n",
    "aete_num_global_attentions = 5 # for attention encoded text embedding\n",
    "aete_dimensionality = 2000 # for attention encoded text embedding\n",
    "num_classes = 14 # number of classes for classification\n",
    "dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = False  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "k = 1 # the number of sequences to sample for each given report  \n",
    "\n",
    "\n",
    "\n",
    "##############\n",
    "\n",
    "encoder = Encoder(encoder_dim)\n",
    "decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
    "                                   embed_dim=emb_dim,\n",
    "                                   decoder_dim=decoder_dim,\n",
    "                                   vocab_size=vocab_size,\n",
    "                                   num_classes=num_classes, \n",
    "                                   aete_num_global_attentions=aete_num_global_attentions, \n",
    "                                   aete_dimensionality=aete_dimensionality,\n",
    "                                   encoder_dim=encoder_dim,\n",
    "                                   dropout=dropout)\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = torch.load(pathModel)\n",
    "encoder.load_state_dict(checkpoint['encoder'])\n",
    "decoder.load_state_dict(checkpoint['decoder'])\n",
    "encoder = encoder.cuda()\n",
    "decoder = decoder.cuda()\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "\n",
    "\n",
    "#-------------------- SETTINGS: DATASET BUILDERS\n",
    "transformList = []\n",
    "transformList.append(transforms.Resize(img_size))\n",
    "transformList.append(transforms.ToTensor())\n",
    "transformList.append(transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]))\n",
    "transformSequence = transforms.Compose(transformList)\n",
    "\n",
    "# DataLoader\n",
    "datasetTest = DatasetGenerator(pathImageDirectory=pathDirDataTest, \n",
    "                                           pathDatasetFile=pathFileTest,\n",
    "                                           transform=transformSequence,\n",
    "                                          report_exists=True)\n",
    "test_loader = DataLoader(dataset=datasetTest, batch_size=1, shuffle=False, \n",
    "                           num_workers=1, pin_memory=True)\n",
    "\n",
    "\n",
    "# For each image\n",
    "outputs = torch.FloatTensor().to(device)\n",
    "labels = torch.FloatTensor().to(device)\n",
    "with torch.no_grad():\n",
    "    for idx, (image, label) in tqdm(enumerate(test_loader)):\n",
    "        with torch.no_grad():\n",
    "            # Move to GPU device, if available\n",
    "            image = image.to(device)  # (1, 3, 224,224)\n",
    "\n",
    "            # Encode\n",
    "            encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "            enc_image_size = encoder_out.size(1)\n",
    "            encoder_dim = encoder_out.size(3)\n",
    "\n",
    "            # Flatten encoding\n",
    "            encoder_out_img = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
    "            num_pixels = encoder_out_img.size(1)\n",
    "\n",
    "            # We'll treat the problem as having a batch size of k\n",
    "            encoder_out = encoder_out_img.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "\n",
    "            # Tensor to store top k previous words at each step; now they're just <start>\n",
    "            k_prev_words = torch.LongTensor([start_token_index] * k).to(device)  # (k,)\n",
    "            \n",
    "            # Tensor to store top k sequences; now they're just <start>\n",
    "            seqs = k_prev_words.unsqueeze(-1)  # (k, 1)\n",
    "\n",
    "            # Tensor to store top k sequences' scores; now they're just 0\n",
    "            top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "\n",
    "            # Lists to store completed sequences and scores\n",
    "            complete_seqs = list()\n",
    "            complete_seqs_scores = list()\n",
    "\n",
    "            # Start decoding\n",
    "            step = 1\n",
    "            h, c = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "            complete_alphas = []\n",
    "            complete_hiddens = [] \n",
    "            alphas = torch.FloatTensor().to(device)\n",
    "            hiddens = torch.FloatTensor().to(device)\n",
    "            # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "            while True:\n",
    "                embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "\n",
    "                awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim) [attention_embedding], (s, num_pixels) [alphas]\n",
    "\n",
    "                gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
    "                awe = gate * awe\n",
    "\n",
    "                h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
    "\n",
    "\n",
    "                #### Now, sample for next words \n",
    "                scores = decoder.fc(h)  # (s, vocab_size)\n",
    "                scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "                # Add\n",
    "                scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "                # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "                if step == 1:\n",
    "                    top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "                else:\n",
    "                    # Unroll and find top scores, and their unrolled indices\n",
    "                    top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "                # Convert unrolled indices to actual indices of scores\n",
    "                prev_word_inds = top_k_words / vocab_size  # (s)\n",
    "                next_word_inds = top_k_words % vocab_size  # (s)\n",
    "\n",
    "                # Add new words to sequences\n",
    "                seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "                # Add new alphas and new hiddens to the sequences \n",
    "                hiddens = torch.cat([hiddens,h.unsqueeze(1)], dim=1) # h.squeeze(1) has dim (s,1,decoder_dim) \n",
    "                alphas = torch.cat([alphas,alpha.unsqueeze(1)],dim=1) # alpha.squeeze(1) has dim (s,1,num_pixels)\n",
    "\n",
    "                # Which sequences are incomplete (didn't reach <end>)?\n",
    "                incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                                   next_word != end_token_index]\n",
    "                complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "                # Set aside complete sequences\n",
    "                if len(complete_inds) > 0:\n",
    "                    complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "                    complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "                    complete_hiddens.extend(hiddens[complete_inds].tolist())\n",
    "                    complete_alphas.extend(alphas[complete_inds].tolist())\n",
    "                    break\n",
    "#                     k -= len(complete_inds)  \n",
    "#                     k = 0 \n",
    "                    \n",
    "#                     if k == 0:\n",
    "#                         print('shouldnt we be here')\n",
    "#                         break\n",
    "\n",
    "                # Proceed with incomplete sequences otherwise\n",
    "                \n",
    "    #             \n",
    "                seqs = seqs[incomplete_inds]\n",
    "                alphas = alphas[incomplete_inds]\n",
    "                hiddens = hiddens[incomplete_inds]\n",
    "                h = h[prev_word_inds[incomplete_inds]]\n",
    "                c = c[prev_word_inds[incomplete_inds]]\n",
    "                encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "                top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "                \n",
    "                k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1) #this causes kernel breakdown\n",
    "#                 k_prev_words = next_word_inds\n",
    "    \n",
    "                # Break if things have been going on too long\n",
    "                if step > max_report_length:\n",
    "                    # Add all unfinished to the complete list \n",
    "                    complete_seqs.extend(seqs[incomplete_inds].tolist())\n",
    "                    complete_seqs_scores.extend(top_k_scores[incomplete_inds])\n",
    "                    complete_hiddens.extend(hiddens[incomplete_inds].tolist())\n",
    "                    complete_alphas.extend(alphas[incomplete_inds].tolist())\n",
    "                    break\n",
    "                step += 1\n",
    "\n",
    "            i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "            seq = complete_seqs[i]\n",
    "            final_hidden = torch.Tensor(complete_hiddens[i]).squeeze(0).cuda()\n",
    "            final_alpha = torch.Tensor(complete_alphas[i]).squeeze(0).cuda()\n",
    "            #Should we write out a report just to see? \n",
    "            ## TODO: write out a report. \n",
    "\n",
    "            #Finish up with AETE and SW_GAP\n",
    "            x_aete, g = decoder.aete(final_hidden.unsqueeze(0), [final_hidden.size(0)])\n",
    "            x_swgap = decoder.swgap(g, final_alpha.unsqueeze(0), encoder_out_img)\n",
    "            x = torch.cat([x_aete, x_swgap], dim=-1)\n",
    "            output = decoder.fc_final(x)\n",
    "            final_sigmoid = nn.Sigmoid()\n",
    "            output = final_sigmoid(output)\n",
    "            outputs = torch.cat([outputs,output], dim=0)\n",
    "            labels = torch.cat([labels,label.cuda()], dim=0)\n",
    "            \n",
    "            if idx % 1000 == 0 and idx != 0:\n",
    "                aucs = computeAUROC(labels, outputs, num_classes)\n",
    "                print('At index %d, the overall AUC is:', aucs[-1])\n",
    "aucs = computeAUROC(labels, outputs, num_classes)\n",
    "print('Overall Test AUC:',aucs[-1],'\\nOther AUCs:',aucs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = KeyedVectors.load(embeddingModelFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "report = [model.index2word[ind-1] for ind in seq[1:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['final',\n",
       " 'report',\n",
       " 'examination',\n",
       " 'chest',\n",
       " 'pa',\n",
       " 'and',\n",
       " 'lat',\n",
       " 'indication',\n",
       " 'year',\n",
       " 'old',\n",
       " 'woman',\n",
       " 'with',\n",
       " 'cough',\n",
       " 'and',\n",
       " 'fever',\n",
       " 'ro',\n",
       " 'pna',\n",
       " 'ro',\n",
       " 'pna',\n",
       " 'impression',\n",
       " 'in',\n",
       " 'comparison',\n",
       " 'with',\n",
       " 'the',\n",
       " 'study',\n",
       " 'of',\n",
       " 'there',\n",
       " 'is',\n",
       " 'little',\n",
       " 'change',\n",
       " 'and',\n",
       " 'no',\n",
       " 'evidence',\n",
       " 'of',\n",
       " 'acute',\n",
       " 'cardiopulmonary',\n",
       " 'disease',\n",
       " 'no',\n",
       " 'pneumonia',\n",
       " 'vascular',\n",
       " 'congestion',\n",
       " 'or',\n",
       " 'pleural',\n",
       " 'effusion']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
